import streamlit as st
import os
import yaml
from utils.bedrock import BedrockClient

def load_transcript_file(file_path: str) -> str:
    """
    Load transcript text from file
    
    Args:
        file_path: Path to the transcript file
        
    Returns:
        Transcript text content
    """
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        else:
            return "ë…¹ì·¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"

def load_prompt_examples(file_path: str = "prompt_examples.yaml") -> dict:
    """
    Load prompt examples from YAML file
    
    Args:
        file_path: Path to the YAML file containing prompt examples
        
    Returns:
        Dictionary containing prompt examples
    """
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as file:
                return yaml.safe_load(file)
        else:
            return {"prompt_examples": {}}
    except Exception as e:
        st.error(f"í”„ë¡¬í”„íŠ¸ ì˜ˆì œ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        return {"prompt_examples": {}}

def main():
    st.set_page_config(
        page_title="ê³ ê°ì„¼í„° ë°ëª¨",
        page_icon="ğŸ“",
        layout="wide"
    )
    
    st.title("ğŸ“ ìë™ì°¨ ë³´í—˜ ìƒë‹´")
    st.markdown("AI-powered call center support system using Amazon Bedrock")

    # Sidebar for configuration
    with st.sidebar:
        st.header("âš™ï¸ Model Configuration")
        
        # Model selection with display names and IDs
        model_options = {
            "Claude Sonnet 4": "apac.anthropic.claude-sonnet-4-20250514-v1:0",
            "Claude 3.7 Sonnet": "apac.anthropic.claude-3-7-sonnet-20250219-v1:0",
            "Claude 3.5 Sonnet v2": "apac.anthropic.claude-3-5-sonnet-20241022-v2:0",
            "Nova Pro": "apac.amazon.nova-pro-v1:0",
            "Nova Lite": "apac.amazon.nova-lite-v1:0"
        }
        
        selected_model_name = st.selectbox(
            "Select Model",
            options=list(model_options.keys()),
            help="Choose the AI model for generating responses"
        )
        
        # Get the actual model ID
        model_id = model_options[selected_model_name]
        
        st.divider()
        
        # Inference parameters
        st.subheader("ğŸ›ï¸ Inference Parameters")
        
        max_tokens = st.slider(
            "Max Tokens",
            min_value=100,
            max_value=4000,
            value=2000,
            step=100,
            help="Maximum number of tokens in the response"
        )
        
        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.0,
            step=0.1,
            help="Controls randomness: lower = more focused, higher = more creative"
        )
        
        top_p = st.slider(
            "Top P",
            min_value=0.0,
            max_value=1.0,
            value=0.9,
            step=0.1,
            help="Controls diversity: lower = more focused, higher = more diverse"
        )
        
        st.divider()
        
        # System message customization
        st.subheader("ğŸ“ System Prompt")
        custom_system_message = st.text_area(
            "Custom System Prompt (optional)",
            placeholder="Enter custom instructions for the AI assistant...",
            height=100,
            help="Override the default system prompt with custom instructions"
        )
    
    with st.expander("ì•„í‚¤í…ì²˜ ë³´ê¸°"):
        st.image("images/call-center-01.png")

    st.audio("media/3_Inquiries_related_to_premium_surcharge_calculation.mp3", format='audio/mp3')

    # ë…¹ì·¨ íŒŒì¼ ê²½ë¡œ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
    transcript_file_path = "media/3_Inquiries_related_to_premium_surcharge_calculation.txt"
    
    # ë…¹ì·¨ í…ìŠ¤íŠ¸ ë¡œë“œ (ì „ì—­ì ìœ¼ë¡œ ì‚¬ìš©)
    transcript_text = load_transcript_file(transcript_file_path)
    
    with st.expander("ë…¹ì·¨ë¡ ë³´ê¸°"):
        # ë…¹ì·¨ ë‚´ìš© í‘œì‹œ
        st.write(transcript_text)

    # Main content area
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("í”„ë¡¬í”„íŠ¸")
        # ë…¹ì·¨ ë‚´ìš©ì´ ë³µì‚¬ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        default_text = ""
        if hasattr(st.session_state, 'transcript_copied'):
            default_text = st.session_state.transcript_copied
            # í•œ ë²ˆ ì‚¬ìš© í›„ ì‚­ì œ
            del st.session_state.transcript_copied
            
        user_input = st.text_area(
            "AIì—ê²Œ ì§€ì‹œë¥¼ ë‚´ë ¤ì£¼ì„¸ìš”.:",
            value=default_text,
            height=250,
            placeholder="Type the customer's inquiry here..."
        )
        
        if st.button("ì‹¤í–‰", type="primary"):
            if user_input:
                try:
                    bedrock_client = BedrockClient()
                    
                    # Prepare inference config
                    inference_config = {
                        "maxTokens": max_tokens,
                        "temperature": temperature,
                        "topP": top_p
                    }
                    
                    # Prepare context-aware prompt with transcript
                    context_prompt = f"""ë‹¤ìŒì€ ê³ ê°ê³¼ ìƒë‹´ì› ê°„ì˜ í†µí™” ë…¹ì·¨ë¡ì…ë‹ˆë‹¤:

<context>
{transcript_text}
</context>

ìœ„ <context>ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ìš”ì²­ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{user_input}"""
                    
                    # Always use streaming response
                    response_placeholder = st.empty()
                    full_response = ""
                    
                    for chunk in bedrock_client.generate_response_stream(
                        prompt=context_prompt, 
                        model_id=model_id,
                        inference_config=inference_config,
                        custom_system_message=custom_system_message if custom_system_message else None
                    ):
                        full_response += chunk
                        response_placeholder.markdown(full_response + " â–‹")
                    
                    response_placeholder.markdown(full_response)
                    st.success("ì‘ë‹µ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                            
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜: {str(e)}")
            else:
                st.warning("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # í”„ë¡¬í”„íŠ¸ ì˜ˆì œë¥¼ ìœ„í•œ ì˜ì—­
    with col2:
        st.subheader("í”„ë¡¬í”„íŠ¸ ì˜ˆì œë“¤")
        
        # YAML íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ì˜ˆì œë“¤ ë¡œë“œ
        prompt_data = load_prompt_examples()
        prompt_examples = prompt_data.get("prompt_examples", {})
        
        # ê° í”„ë¡¬í”„íŠ¸ ì˜ˆì œë¥¼ expanderì™€ st.codeë¡œ í‘œì‹œ
        for key, example in prompt_examples.items():
            title = example.get("title", key)
            content = example.get("content", "")
            
            with st.expander(title):
                st.code(content, language="text")
        

if __name__ == "__main__":
    main()